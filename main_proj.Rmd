---
title: "Exploring Mac Miller's Discography with Spotify API"
author: "Collin Smith"
date: '2022-05-19'
output: 
  github_document:
    toc: true
    toc_depth: 4
    df_print: "kable"
---

### Welcome!

Hello, and welcome to my very first Markdown publication. I am still relatively 
new to R, with about 2 years of exposure and experience with the language. 
The vast majority of my time spent with R has been for university courses. 
Throughout this time I have learned a great deal from books like 
Hadley Wickham's [R for Data Science](https://r4ds.had.co.nz/), as well as 
tutorials, articles, and markdown pages from sites such as 
[RPubs](https://rpubs.com/), [R-bloggers](https://www.r-bloggers.com/), 
and [Towards Data Science](https://towardsdatascience.com/). 
My hope is that this project can in turn be used to help someone else down 
the road, as others have helped me.

In this project, we will be exploring how to use Spotify's API and 
the **spotifyr** package to access data about a particular artists discography. 
We'll also be utilizing the **genius** package to retrieve lyrics for 
songs, allowing for sentiment to be explored and visualized.

### Obtaining Spotify API Credentials

To begin, we will first need to head over to the 
[Spotify for Developers](https://developer.spotify.com/dashboard/) page, 
where we will be registering an application to obtain an API key. Once you've 
logged in, select "Create an App" and fill out the required fields. Completing 
this will give you access to two important fields: your **client id** and 
your **client secret** (or API key). These fields will be used to let the
API know who is accessing it and that you have proper authentication. 
To get started, first install the **spotifyr** package if you don't already
have it.

```{r pkg-install, eval = F, include = T}
install.packages("spotifyr")
```

Next, we can use the following code to pass out authentication credentials
to the API, giving us access.

```{r auth-example, eval = F, include = T}
Sys.setenv(SPOTIFY_CLIENT_ID = 'your client id')
Sys.setenv(SPOTIFY_CLIENT_SECRET = 'your client secret')
access_token <- get_spotify_access_token()
```

```{r auth-actual, eval = T, echo = F}
source('auth_func.R')
authenticate()
```

### Gathering the Data

Now that you are authenticated with the API, we can begin using the 
**spotifyr** package's functions to retrieve information.
In this project, we will be analyzing Mac Miller's discography and exploring 
how his sound changed throughout his career. To get information regarding 
a single artist's discography, we can use the `get_artist_audio_features()` 
function. The function will return a dataframe containing information about all 
of the artists' music that is hosted on Spotify. The function takes many 
possible arguments, but for the sake of this project, we only need to pass in 
one: the name of the artist for which we wish to get information.

```{r get-data, echo = T}
library(spotifyr)
mm_data <- get_artist_audio_features(artist = "Mac Miller")
```

To check that this function performed as expected, let's take a very quick
glance at the returned dataframe.

```{r data-check, echo = T}
colnames(mm_data)
unique(mm_data$album_name)
head(mm_data$track_name, 15)
dim(mm_data)
```

Excellent! The returned dataframe contains ``r dim(mm_data)[1]`` observations,
or in this case songs, and each observation has ``r dim(mm_data)[2]`` variables.

### Checking the Data

We can see from the `unique(mm_data$album_name)` function that the albums are
listed in order of upload date, with Faces being the most recent album added
to Mac's Spotify page. While this may seem handy, it is important to note that
the order in which albums are uploaded to Spotify is not always the same order
that the albums were released. We can observe this by taking a look at the
`album_release_date` variable.

```{r libs, message = F}
library(tidyverse)
library(plotly)
```

```{r album-dates, echo = T}
mm_data %>% 
  select(album_name, album_release_date) %>%
  distinct()
```

This readout implies that Faces is the most recent album to release. However,
by checking Mac's 
[discography](https://en.wikipedia.org/wiki/Mac_Miller_discography), we can see
that Faces was actually released as a mixtape back in 2014, much earlier than
the variable from Spotify's data would suggest. Since this project involves 
analyzing how Mac's music changed throughout the duration of his career, it is
important to have accurate ordering of dates associated with the albums.

#### Webscraping

To remedy this, we can use some quick web scraping to pull the release dates 
from the wiki page and amend the data.

```{r wiki-scrape, message = F}
# run install.packages('rvest') if you don't have this package already
library(rvest)
url <- "https://en.wikipedia.org/wiki/Mac_Miller_discography"
wp <- read_html(url)
rel_dates <- html_nodes(
  wp, "th i , .plainrowheaders th+ td li:nth-child(1) , th i a") %>%
  html_text()
```

Now, the above code may look a little intimidating if you are new to
webscraping. That's okay, it looks much scarier than it really is. The
`read_html()` function simply reads the page's html code and stores it as a 
list in your R environment. Then, we need to tell R what parts of the webpage
we want extracted. To do this, I used the 
[SelectorGadget](https://chrome.google.com/webstore/detail/selectorgadget/mhjhnkcfbdhnjickkkdbjoemdmbfginb?hl=en) extension for Chrome. Using the tool makes webscraping
very simple, you just highlight the elements you wish to capture and the tool
will give you the CSS selector for it. That is how the arguments you see in the
`html_nodes()` function were found. Once you've identified the nodes, pass the
results to the `html_text()` function and voila! You know have text from a
website stored right in your R environment.

Let's check out what our scraping resulted in:

```{r chk-scrape}
head(rel_dates, 20)
```

#### Cleaning the scraped data

While that looks pretty good, you can see that the album titles are listed
twice, and the release dates could be formatted a little nicer. Let's fix that
up to a nicer format.

```{r scrape-cleaning}
# Remove duplicates and format into dataframe for manipulation
rel_dates <- matrix(
  unique(rel_dates), ncol = 2, byrow = T) %>%
  as.data.frame()

# Filter out any works that aren't hosted on Spotify
rel_dates <- rel_dates %>% filter(
  V1 %>% tolower() %in% (unique(mm_data$album_name) %>% 
                    gsub("( \\().*", "", .) %>%
                    tolower()))

# Cleaning date text
rel_dates$V2 <- gsub("(?:Released: )", "", rel_dates$V2)
rel_dates$V2 <- gsub(".{4}$", "", rel_dates$V2)

# Converts textual dates to date type object
rel_dates$V2 <- lubridate::parse_date_time(
  rel_dates$V2, orders = "mdy") %>%
  lubridate::as_date()

# Check results to ensure they look as expected
rel_dates
```

#### Prepping for Merge

Now our release dates are in a more workable format. However, before we merge
the two datasets, let's first take one more look at the album names in our 
main dataframe, `mm_data`. 

```{r album-name-check}
unique(mm_data$album_name)
```

We see from this readout that many albums contain multiple editions, such as
Deluxe releases, remasters, or commentary bonuses. To prevent our analysis from
being biased towards those repeated works, we should selectively filter out 
albums that are listed multiple times. Firstly, the commentary version of
Blue Slide Park will be omitted. Next, for any album that has a deluxe release,
we will keep only the deluxe release, dropping the original album from the
dataset. Lastly, we will rename 
*Best Day Ever (5th Anniversary Remastered Edition)* and
*Macadelic (Remastered Edition)*. 
This will help us when merging the datasets.

```{r pre-merge-cleaning}
# Shortening 'Best Day Ever' album name for merging
mm_data$album_name[
  mm_data$album_name == 
  "Best Day Ever (5th Anniversary Remastered Edition)"] = "Best Day Ever"

# Dropping '(Remastered Edition)' from Macadelic
mm_data$album_name[
  mm_data$album_name == 
  "Macadelic (Remastered Edition)"] = "Macadelic"

# Drop any non-deluxe editions of albums that have deluxe editions
# Note that Blue Slide Park's additional versions were also dropped
# Live From London was dropped as it only included already present songs
mm_data <- filter(mm_data, !(album_name %in% c(
  "Circles", 
  "Watching Movies with the Sound Off", 
  "K.I.D.S.",
  "Mac Miller : Live From London (With The Internet)",
  "Blue Slide Park (Commentary Version)",
  "Blue Slide Park (Edited Version)")))

# Adding " (Deluxe)" onto album names in true release date set for merging
for(i in c(2, 6, 8)){
  if(i == 2){
    rel_dates$V1[i] = str_c(rel_dates$V1[i], " (Deluxe Edition)")}
  else
    rel_dates$V1[i] = str_c(rel_dates$V1[i], " (Deluxe)")}

# Adjusting Capitalization of "Live from Space" to match mm_data$album_name
rel_dates$V1[rel_dates$V1 == "Live from Space"] = "Live From Space"
```

#### Merging the Data

Now that our extra versions have been dropped from the data, we can finally
merge our two datasets to attach the accurate release dates.

```{r date-combine}
# Performing the merge of the two datasets
mm_data <- left_join(mm_data, rel_dates,
                     by = c("album_name" = "V1"))
mm_data <- mm_data %>% rename("true_rel_date" = "V2")
```

With the merge complete, let's take a look at the differences between the
original `album_release_date` column and our new `true_rel_date` column.

```{r rel-date-comp}
select(mm_data, album_release_date, album_name, true_rel_date) %>%
  unique()
```

Excellent! Now that we have each album's true release date, we can go ahead and
drop the old variable `album_release_date` and rename our new variable to take
its place.

```{r rel-date-swap}
mm_data <- select(mm_data, -album_release_date) %>% 
  rename(album_release_date = true_rel_date)
```

### Filtering the Data

Now that our release date issues have been solved, let's take a look at which
variables we'd like to keep, and which ones we can exclude moving forward.

```{r df-name-list}
print(names(mm_data))
```

As we can see, there are quite a few variables in the set. To get an idea of
what each variable holds, we can use *tidyverse*'s `glimpse()` function.

```{r data-glimpse}
glimpse(mm_data)
```

Wow! That's a lot of info. To make things a little simpler, we can refer to
[Spotify's API documentation](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-several-audio-features)
to get a better idea of what these variables represent. Some variables are 
rather self-explanatory, such as `artist_name`, `track_name`, `album_name`, and
`duration_ms`. For this analysis, we want to keep identifying information such
as a tracks name, the album it came from, and the release date. The other
variables we'll want to keep are measures about the songs sonic signature. These
measures include `danceability`, `energy`, `loudness`, `speechiness`,
`acousticness`, `instrumentalness`, `liveness`, and `valence`. These are metrics
provided by Spotify that give quantitative measures of a track's audio
characteristics. More information on these metrics and how they are derived can
be found at [Spotify's API documentation](https://developer.spotify.com/documentation/web-api/reference/#/operations/get-several-audio-features).
Other variables we'll want to keep are `key`, `mode`, `tempo`, `time_signature`,
`duration_ms`, and `explicit`. These variables provide more information about
the composition of the song.

Now that we've identified the variables we want to keep in our working data, we
can go ahead and create a filtered version of the full data to move forward
with. We can also take this opportunity to reorder our variables into more 
organized groups.

```{r select-variables}
df <- mm_data %>% select(
  # identifying information
  track_name,
  artist_name,
  album_name,
  album_release_date,
  # spotify provided quantitative measures
  acousticness,
  danceability,
  energy,
  instrumentalness,
  liveness,
  loudness,
  speechiness,
  valence,
  # composition information
  duration_ms,
  explicit,
  key,
  mode,
  tempo,
  time_signature
)
```

### Exploratory Data Analysis (EDA)

With the data sorted and filtered to only include relevant variables, we are
now ready to begin using statistical summaries and creating visualizations
to glean insights from. We will be exploring the data to discover patterns, 
identify anomalies (or outliers), and to make some informed observations that
will help us to better understand the dataset.