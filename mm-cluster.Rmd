---
title: "Spotify Data Clustering"
author: "Collin Smith"
date: '2022-07-09'
---

# Welcome!

This is the third and final section of my introductory look at exploring the
Spotify API using R. If you haven't already read through sections one and two,
where we cover accessing, cleaning, and visualizing the data, I'd strongly
recommend you begin with those before reading this section. If you're coming
from there, welcome back, and let's continue!

In this section we will be applying a clustering algorithm to the data we've 
collected to see if any interesting patterns emerge in the results. We'll be
using k-means clustering, probably the most common clustering algorithm out,
and for good reason! It's simple, easy to interpret, and not too computationally
expensive. All around, a very good place to start, so let's go ahead and dive 
right in!

# Standarding the Data

Before we can actually apply the clustering, we first need to ensure that all
of the data we'll be working with has been scaled to have the same range. This
helps the clustering algorithm treat each variable equally instead of weighting
certain variables higher or lower when determining what cluster an observation
should be placed in. To do this, we'll be applying a technique called min-max
normalizing, which will preserve the distribution of the measurements while
transforming them to fall within the 0 to 1 range. 

```{r libs-setup, echo = F, message = F}
knitr::opts_chunk$set(fig.align = "center")
ggplot2::theme_set(ggplot2::theme_minimal())
library(tidyverse)
```

```{r read-data}
df <- read_csv("section-two-end-df.csv", show_col_types = F)
```

```{r min-max-normalize}
min_max <- function(x, na.rm = TRUE){
  return((x - min(x)) / (max(x) - min(x)))
}
```

To check if our function does indeed retain the original distribution, we can
use some dummy data to perform a quick analysis.

```{r normalize-function-check}
dummy_data <- rnorm(500, mean = -3, sd = 20)
y <- dnorm(dummy_data, mean(dummy_data), sd(dummy_data))
plot(x = dummy_data, y = y,
     xlab = "Dummy Data",
     ylab = "Density")
```

```{r dummy-data-after-norm}
dummy_data <- min_max(dummy_data)
y <- min_max(y)

plot(x = dummy_data, y = y,
     xlab = "Dummy Data",
     ylab = "Density",
     main = "Dummy Data after Normalization")
```

As you can see, the distribution of points is exactly the same. However, pay
special attention to the axes. Both the y and x axis range from 0 to 1 exactly.
This means our normalization function works as expected!

```{r apply-norm}
# recode TRUE/FALSE to binary indicator (1 == TRUE)
df$explicit <- ifelse(df$explicit == TRUE, 1, 0)

# normalize the numeric columns in the dataset
df[,5:17] <- df[,5:17] %>% apply(., 2, function(x) min_max(x))

# check the results of the normalization
summary(df[,5:17])
```

Excellent! As we can see from the `summary()` readout, each of our numeric
columns now has a minimum of 0 and a maximum of 1. This means each variable
will carry equal weighting in the clustering, which is exactly what we wanted
to achieve.

```{r cluster-libs, message = F}
library(factoextra)
```

# Applying the Clustering

When using k-means clustering, the user needs to predetermine the number of 
clusters they'd like the algorithm to sort into. This decision can have a very
significant impact on the efficacy of the clustering, and as should typically
be chosen after considering a couple different values. A common method for 
determining the optimal number of clusters (also known as k-value) is using
an [elbow plot](https://uc-r.github.io/kmeans_clustering). This sort of plot
shows the total WSS (within-cluster sum of square) variation for each value of
k considered. The idea is to pick the k-value where the plot "bends" (aka the
point of the elbow). The k-value associated with this point is the number of 
clusters that produces the lowest total WSS without passing into the realm of
diminishing returns. Theoretically, it is possible to have a total WSS of 0.
However, this is only the case when $k = n$, where $n = # of Observations$. 
This extreme case is basically useless, as each observation would be placed into
its own cluster. For this project, we'll consider k-values ranging from 2 to 15.

```{r elbow-plot}
set.seed(333)

fviz_nbclust(df[,5:17], 
             kmeans, 
             method = "wss",
             k.max = 15)
```

While not always incredibly easy to interpret, I would say that k = 6 is the
most reasonable choice based on the elbow plot.

```{r finalize-cluster}
set.seed(333)
clustering <- kmeans(df[,5:17],
                     centers = 6,
                     nstart = 25,
                     iter.max = 25)
set.seed(NULL)

print(clustering)
```

```{r cluster-viz}
fviz_cluster(clustering, data = df[,5:17])
```

From this visualization, we can see that there is quite a bit of overlap towards
the left-hand side of the plot. Let's generate some visualizations using a lower
k-value and see how they differ.

```{r cluster-viz-compare}
set.seed(333)

k2 <- kmeans(df[,5:17], centers = 2, nstart = 25)
k3 <- kmeans(df[,5:17], centers = 3, nstart = 25)
k4 <- kmeans(df[,5:17], centers = 4, nstart = 25)
k5 <- kmeans(df[,5:17], centers = 5, nstart = 25)

plot2 <- fviz_cluster(k2, data = df[,5:17])
plot3 <- fviz_cluster(k3, data = df[,5:17])
plot4 <- fviz_cluster(k4, data = df[,5:17])
plot5 <- fviz_cluster(k5, data = df[,5:17])

gridExtra::grid.arrange(plot2, plot3, plot4, plot5, nrow = 2)
```

These plots don't look too different than our k = 6 plot. Furthermore, our
`print(clustering)` statement from before told us that our 
$between\_ss \ /\  total\_ss = 52.9%$. A value this low tends to indicate that
the k-means clustering simply didn't work with the data. Sure, clusters were
created, but they're so loosely defined that they aren't worth keeping. It's
very possible that we are feeding too many variables into the algorithm. To
overcome this, we can use a technique called *Principal Component Analysis*, or
PCA for short. 

# Principal Component Analysis

When working with data, it is not uncommon for the majority of information in
the dataset to be contained within just a few different variables, or at least
in their interactions. In other words, when working with high dimensionality
data, it is commonly possible for most of the information within that data to
be stored using far fewer dimensions. However, figuring out how to effectively
maintain that information while shrinking the size of its representation is not
always an easy task. Analyzing correlations between variables is a great place
to start, as covered at the end of section two of this series. However, that
is not always enough. This is where PCA comes in.

Principal component analysis is the process of computing the principal
components of a collection of observations, and using them to perform a change
of basis on that data in order to reduce the number of features in the data.
To remove features, PCA calculates the amount of variance in the data that can
be explained by the $i^{th}$ variable in the set. The first principal component
explains the greatest amount of variation, and so on until all variables have 
been assessed. Once that is done, it is typically up to the user to decide how
many of those variables within the set of *i* variables to retain in their new
set. Often, an elbow plot is used to help guide said decision.

```{r pca-reduce}
inputs <- df[,5:17] # create input matrix of only numeric variables

inputs_pca <- prcomp(inputs, 
                     center = FALSE, # data is 
                     scale. = FALSE) # already normalized

summary(inputs_pca)
```

This tells us that by just using the first two principal components, we are able
to explain 88.36% of the variation in the data. We can further analyze the 
results of the PCA using a biplot.

```{r pca-biplot}
album_palette = c(
  "K.I.D.S. (Deluxe)" = "#387228",
  "Best Day Ever" = "#C315AA",
  "Blue Slide Park" = "#3540F2",
  "Macadelic" = "#767092",
  "Watching Movies with the Sound Off (Deluxe Edition)" = "#DA252A",
  "Live From Space" = "#FE675C",
  "Faces" = "#FDBB1E",
  "GO:OD AM" = "#A2A2A2",
  "The Divine Feminine" = "#DDC1BE",
  "Swimming" = "#668099",
  "Circles (Deluxe)" = "#464646"
  )

library(ggbiplot)
biplot <- ggbiplot(inputs_pca,
                   obs.scale = 1,
                   var.scale = 1,
                   groups = factor(df$album_name),
                   ellipse = TRUE,
                   circle = TRUE,
                   ellipse.prob = 0.68)

biplot <- biplot + 
  scale_color_manual(values = album_palette, name = "") +
  theme(legend.position = "none")

print(biplot)
```

This biplot tells us that nearly all the variables in the data are relatively
highly correlated. It is also interesting to note that PC1 is negatively 
correlated with every single variable. None of the arrows point in the positive
x direction, indicating that there is no positive correlation between any of 
the variables and PC1. Furthermore, we see that two, maybe three
distinct groups of points form. This suggests that if we were to apply a 
clustering algorithm where the number of clusters requires choosing, 
two or three clusters may yield interesting results.