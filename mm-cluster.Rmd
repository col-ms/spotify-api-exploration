---
title: "Spotify Data Clustering"
author: "Collin Smith"
date: '2022-07-09'
---

# Welcome!

This is the third and final section of my introductory look at exploring the
Spotify API using R. If you haven't already read through sections one and two,
where we cover accessing, cleaning, and visualizing the data, I'd strongly
recommend you begin with those before reading this section. If you're coming
from there, welcome back, and let's continue!

In this section we will be applying a clustering algorithm to the data we've 
collected to see if any interesting patterns emerge in the results. We'll be
using k-means clustering, probably the most common clustering algorithm out,
and for good reason! It's simple, easy to interpret, and not too computationally
expensive. All around, a very good place to start, so let's go ahead and dive 
right in!

# Standarding the Data

Before we can actually apply the clustering, we first need to ensure that all
of the data we'll be working with has been scaled to have the same range. This
helps the clustering algorithm treat each variable equally instead of weighting
certain variables higher or lower when determining what cluster an observation
should be placed in. To do this, we'll be applying a technique called min-max
normalizing, which will preserve the distribution of the measurements while
transforming them to fall within the 0 to 1 range. 

```{r libs-setup, echo = F, message = F}
knitr::opts_chunk$set(fig.align = "center")
ggplot2::theme_set(ggplot2::theme_minimal())
library(tidyverse)
```

```{r read-data}
df <- read_csv("section-two-end-df.csv", show_col_types = F)
```

```{r min-max-normalize}
min_max <- function(x, na.rm = TRUE){
  return((x - min(x)) / (max(x) - min(x)))
}
```

To check if our function does indeed retain the original distribution, we can
use some dummy data to perform a quick analysis.

```{r normalize-function-check}
dummy_data <- rnorm(500, mean = -3, sd = 20)
y <- dnorm(dummy_data, mean(dummy_data), sd(dummy_data))
plot(x = dummy_data, y = y,
     xlab = "Dummy Data",
     ylab = "Density",
     main = "Dummy Data before Normalization")
```

```{r dummy-data-after-norm}
dummy_data <- min_max(dummy_data)
y <- min_max(y)

plot(x = dummy_data, y = y,
     xlab = "Dummy Data",
     ylab = "Density",
     main = "Dummy Data after Normalization")
```

As you can see, the distribution of points is exactly the same. However, pay
special attention to the axes. Both the y and x axis range from 0 to 1 exactly.
This means our normalization function works as expected!

```{r apply-norm}
# recode TRUE/FALSE to binary indicator (1 == TRUE)
df$explicit <- ifelse(df$explicit == TRUE, 1, 0)

# normalize the numeric columns in the dataset
df[,5:17] <- df[,5:17] %>% apply(., 2, function(x) min_max(x))

# check the results of the normalization
summary(df[,5:17])
```

Excellent! As we can see from the `summary()` readout, each of our numeric
columns now has a minimum of 0 and a maximum of 1. This means each variable
will carry equal weighting in the clustering, which is exactly what we wanted
to achieve.

```{r cluster-libs, message = F}
library(factoextra)
```

# Applying the Clustering

When using k-means clustering, the user needs to predetermine the number of 
clusters they'd like the algorithm to sort into. This decision can have a very
significant impact on the efficacy of the clustering, and as should typically
be chosen after considering a couple different values. A common method for 
determining the optimal number of clusters (also known as k-value) is using
an [elbow plot](https://uc-r.github.io/kmeans_clustering). This sort of plot
shows the total WSS (within-cluster sum of square) variation for each value of
k considered. The idea is to pick the k-value where the plot "bends" (aka the
point of the elbow). The k-value associated with this point is the number of 
clusters that produces the lowest total WSS without passing into the realm of
diminishing returns. Theoretically, it is possible to have a total WSS of 0.
However, this is only the case when $k = n$, where $n = # of Observations$. 
This extreme case is basically useless, as each observation would be placed into
its own cluster. For this project, we'll consider k-values ranging from 2 to 15.

```{r elbow-plot}
set.seed(333)

fviz_nbclust(df[,5:17], 
             kmeans, 
             method = "wss",
             k.max = 15)
```

While not always incredibly easy to interpret, I would say that k = 6 is the
most reasonable choice based on the elbow plot.

```{r finalize-cluster}
set.seed(333)
clustering <- kmeans(df[,5:17],
                     centers = 6,
                     nstart = 25,
                     iter.max = 25)
set.seed(NULL)

print(clustering)
```

```{r cluster-viz}
fviz_cluster(clustering, data = df[,5:17])
```

From this visualization, we can see that there is quite a bit of overlap towards
the left-hand side of the plot. Let's generate some visualizations using a lower
k-value and see how they differ.

```{r cluster-viz-compare}
set.seed(333)

k2 <- kmeans(df[,5:17], centers = 2, nstart = 25)
k3 <- kmeans(df[,5:17], centers = 3, nstart = 25)
k4 <- kmeans(df[,5:17], centers = 4, nstart = 25)
k5 <- kmeans(df[,5:17], centers = 5, nstart = 25)

plot2 <- fviz_cluster(k2, data = df[,5:17])
plot3 <- fviz_cluster(k3, data = df[,5:17])
plot4 <- fviz_cluster(k4, data = df[,5:17])
plot5 <- fviz_cluster(k5, data = df[,5:17])

gridExtra::grid.arrange(plot2, plot3, plot4, plot5, nrow = 2)
```

These plots don't look too different than our k = 6 plot. Furthermore, our
`print(clustering)` statement from before told us that our 
$between\_ss \ /\  total\_ss = 52.9%$. A value this low tends to indicate that
the k-means clustering simply didn't work with the data. Sure, clusters were
created, but they're so loosely defined that they aren't worth keeping. It's
very possible that we are feeding too many variables into the algorithm. To
overcome this, we can use a technique called *Principal Component Analysis*, or
PCA for short. 

# Principal Component Analysis

When working with data, it is not uncommon for the majority of information in
the dataset to be contained within just a few different variables, or at least
in their interactions. In other words, when working with high dimensionality
data, it is commonly possible for most of the information within that data to
be stored using far fewer dimensions. However, figuring out how to effectively
maintain that information while shrinking the size of its representation is not
always an easy task. Analyzing correlations between variables is a great place
to start, as covered at the end of section two of this series. However, that
is not always enough. This is where PCA comes in.

Principal component analysis is the process of computing the principal
components of a collection of observations, and using them to perform a change
of basis on that data in order to reduce the number of features in the data.
To remove features, PCA calculates the amount of variance in the data that can
be explained by the $i^{th}$ variable in the set. The first principal component
explains the greatest amount of variation, and so on until all variables have 
been assessed. Once that is done, it is typically up to the user to decide how
many of those variables within the set of *i* variables to retain in their new
set. Often, an elbow plot is used to help guide said decision.

```{r pca-reduce}
inputs <- df[,5:17] # create input matrix of only numeric variables

inputs_pca <- prcomp(inputs, 
                     center = TRUE, 
                     scale. = FALSE) # data is already normalized

summary(inputs_pca)
```

This tells us that by just using the first two principal components, we are able
to explain 46.3% of the variation in the data. We can further analyze the 
results of the PCA using a biplot.

```{r pca-biplot, message = F}
album_palette = c(
  "K.I.D.S. (Deluxe)" = "#387228",
  "Best Day Ever" = "#C315AA",
  "Blue Slide Park" = "#3540F2",
  "Macadelic" = "#767092",
  "Watching Movies with the Sound Off (Deluxe Edition)" = "#DA252A",
  "Live From Space" = "#FE675C",
  "Faces" = "#FDBB1E",
  "GO:OD AM" = "#A2A2A2",
  "The Divine Feminine" = "#DDC1BE",
  "Swimming" = "#668099",
  "Circles (Deluxe)" = "#464646"
  )

library(ggbiplot)
biplot <- ggbiplot(inputs_pca,
                   obs.scale = 1,
                   var.scale = 1,
                   groups = factor(df$album_name),
                   ellipse = TRUE,
                   circle = TRUE,
                   ellipse.prob = 0.68)

biplot <- biplot + 
  scale_color_manual(values = album_palette, name = "") +
  theme(legend.position = "none")

print(biplot)
```

This biplot tells us that **mode** is highly negatively correlated with PC1, as
indicated by the arrow pointing nearly horizontally to the left. We can also see
that **acousticness** and **instrumentalness** are influenced similarly by PC2.
This makes sense, as those two measures both deal with the presence of
non-electric (acoustic) instruments in a track's sound. We can also see that
the majority of the observations fall into two relatively neat clusters when
mapped using PC1 and PC2. The circles around them represent album groupings. 
These circles tell us that there is a great deal of overlap between most of the
albums' measures, with *Circles* containing most of the outliers up at the top.

```{r, message = F}
library(dplyr)
library(FactoMineR)
```

```{r new-pca}
new_inputs_pca <- PCA(inputs,
                      scale.unit = FALSE,
                      graph = F,
                      ncp = 10)

new_inputs_pca$eig
```

In order to tolerate no more than 15% information loss, we have to use 7 
principal components (cumulative variance prop. of 87.05401). 

```{r}
plot.PCA(new_inputs_pca,
         choix = "ind",
         habillage = 9,
         select = "contrib 5",
         invisible = "quali")
```

From this plot we find 5 row IDs that are considered to be outliers:

note: really, these are simply the 5 observations farthest from the origin
of the PCA plot, but they are worth exploring to identify potential similarities
among them)

* 29
* 36
* 37
* 40
* 129

Let's take a quick look at those observations to see what values stand out.

```{r outlier-obs}
print(df[c(29, 36, 37, 40, 129),])
```

Unsurprisingly, 3/5 points come from the *Circles* album, which has
consistently had the most distinct sound in the dataset. *Good News* and *Surf*
in particular have much higher **instrumentalness** values than the other 
observations. It is very likely that this plays a role in them being grouped
outside the majority of the observations. Additionally, all 3 of the points
from *Circles* have pretty high **danceability** measures. All 5 of the points
have fairly similar **acousticness** measures. This seems like the most likely
contributor for these points being grouped so closely in the above plot.

```{r pca-var-plot}
# get principal component descriptions
pca_dimdesc <- dimdesc(new_inputs_pca)

# get info of PC1 influential features
pca_dimdesc$Dim.1
```

The first principal component gets its information from the **mode** and **key**
variables, both of which can be thought of as factor variables rather than
quantitative measures. This is important knowledge, as it tells us that our
data is being assessed largely by a set of factors. That's not really what we
want, as we want our clustering to be based more so on the quantitative measures
within the set. Let's create a new set of input variables that contains only
the quantitative measures. Additionally, we'll pull the **energy** variable back
to see if we were perhaps too eager to drop that variable from the set.

```{r fresh-df}
# read in fresh dataset (includes energy variable)
df <- read_csv("working-data.csv", show_col_types = FALSE)

# example observations from freshly read in dataframe
print(df[sample(1:nrow(df), 10),])
```

```{r new-inputs-df}
# selecting desired variables for fresh input matrix
inputs <- select(df, 
                 acousticness,
                 danceability,
                 energy,
                 instrumentalness,
                 liveness,
                 loudness,
                 speechiness,
                 valence,
                 duration_ms,
                 tempo)

# input matrix summary statistics
summary(inputs)
```

```{r}
inputs <- scale(inputs) # we use z-score scaling here to minimize outlier impact
```

# Clustering (Take 2)

Now that we're working with a different set of input variables, and have changed
our scaling method, let's try apply the k-means algorithm once again and see
if our results are more promising this time around.

```{r kmeans-v2}
# elbow plot using refreshed input matrix
fviz_nbclust(inputs,
             kmeans,
             method = "wss",
             k.max = 20)
```

The ideal number of clusters according to the above plot looks to be 8. Using
this information, we'll fit a model with 8 centers, being sure to set the 
`nstart` parameter to 25 to allow the function to generate 25 initial clustering
assignments and choose the path that led to the best result.

```{r new-clustering-model}
# set seed for reproducability
set.seed(92)

# fit clustering model
cluster <- kmeans(inputs,
                  centers = 8,
                  iter.max = 25,
                  nstart = 25)

# display clustering result summary
print(cluster)
```

```{r cluster-viz-new}
# visualize clustering
fviz_cluster(cluster,
             geom = "point",
             data = inputs,
             palette = "Dark2",
             main = "K Means Clustering (k = 8)",
             alpha = 0.75)
```

Now that our clustering is complete, we can take the assignment vector and 
append it the original data, allowing for descriptive statistics at the cluster
level to be performed. 

```{r}
# append assignment vector to original dataframe
df$cluster <- cluster$cluster

# print example df with cluster column shown
print(
  select(df, track_name, album_name, cluster) %>% 
  .[sample(1:nrow(df), 10),]
)
```

# Clustering Assessment

Now that our clustering assignment is complete, we should take a look at some
metrics to understand how our model performed on the data.

## Sum of Squares

The **BSS** of the model is `r cluster$betweenss`.

Interpreting this value:

* Between Sum of Squares, or BSS, is a metric that reports the sum of the
  squared distance between each cluster's centroid.
* Generally speaking, **the higher the BSS, the better**, as that indicates the
  clusters are distinct and the centroids are far apart from one another
* A large BSS value suggests that characteristics of the data within each
  cluster are unique and can easily be identified from one another
* A common k-means performance metric is $BSS\ /\ TSS$, where $TSS$ is the
  **Total Sum of Squares**. To get a high value from this equation, we would
  need to raise the number of clusters. However, because we chose the number
  of clusters using the elbow plot method (at k = 8), we won't alter the model
  to optimize the $BSS\ /\ TSS$ metric, and will stay with k = 8.
  
```{r bss-over-tss}
# print rounded bss / tss (as percentage)
round(cluster$betweenss / cluster$totss, 4) * 100
```

This value essentially tells us that each cluster is roughly 53.32% different
than the other clusters. Since the goal here is to build playlists from the
clusters, this value seems like a pretty ideal result. We wouldn't want (or
expect for that matter, since all observations came from a single artist) this
value to be incredibly high, as that would mean that each playlist created would
be drastically different from the others. 

## Prediction Strength

The prediction strength of a clustering assignment refers to the 
generalizability of the model. In short, it asks the question "How well can
this clustering react to observations it hasn't seen before?". It is sometimes
used a metric for validating the choice of k. Common rule of thumb suggests that
the largest number of clusters that leads to a prediction strength of 0.8 or 0.9
should be used. Here we'll use it to assess our choice of k = 8.

```{r pred-str-assess}
# create prediction strength assessment
fpc::prediction.strength(inputs,
                         Gmin = 2,
                         Gmax = 10,
                         M = 100,
                         cutoff = 0.8)
```

We see from this result that the model does not have high prediction strength at
all. In fact, the largest number of clusters that produces a prediction strength
larger than our cutoff of 0.8 is simply 1, which will always have a prediction
strength of 1. This suggests that our data may not vary enough to identify 
distinct clusters. This is likely due to the fact the the data comes entirely
from a single artist. Perhaps including a wider variety of music in the dataset
would allow for more accurate predictions to be made. Nonetheless, prediction
was never the goal of this project, so we are lucky enough to be able to "write
off" the results of this test. However, it is very important to understand what
these results mean and how to interpret them in the context of your data.

# Cluster Analysis

```{r detach-old-libs, include = F}
detach("package:ggbiplot", unload = TRUE)
detach("package:plyr", unload = TRUE)
```

We can first get some rudimentary information about each cluster by taking a 
look at the mean value for each of our input variables, along with the number
of tracks within each cluster.

```{r cluster-mean-explore}
# display table of cluster means
df %>%
  group_by(cluster) %>%
  select(cluster, colnames(inputs)) %>%
  summarise(across(.fns = mean),
            count = n())
```

Some observations from the above readout:

* Cluster 4 has a substantially higher average **acousticness** measure than
  the other clusters
* There doesn't appear to be a great amount of variation regarding the average
  **danceability** values across the clusters (cluster 7 shows highest)
* **energy** sees both drastic differences and stark similarities across each
  cluster. For example, clusters 2 and 5 have extremely similar values, yet
  clusters 4 and 5 have very different values
* Average **instrumentalness** is low across the board
* Average **liveness** values are, again, relatively low across the board, save
  for cluster 5. This suggests that cluster 5 would contain many tracks from
  *Live From Space*
* Decent variation in average **loudness** values
* Pretty similar groupings in average **speechiness** values, though values
  across the board are low
* Average **valence** values display a pretty good amount of variation across
  all clusters
* Average **duration_ms** values show that 50% of clusters (1, 2, 7, 8) have
  very similar measures, with a couple outliers. The drastic difference among
  those outliers suggests this variable may have been rather influential in the
  clustering assignment
* **Tempo** averages see a good amount of variation
* **count**, the size of each cluster, varies greatly from 3 to 36. It would be
  interesting to examine how many albums are represented in each cluster
  
```{r albums-in-cluster}
# examine number of albums represented by cluster
albums_in_clusters_plot <- df %>%
  group_by(cluster) %>%
  summarise(n_albums = n_distinct(album_name)) %>%
  ggplot(aes(factor(cluster), n_albums)) +
  geom_col() +
  labs(y = "Count",
       title = "How Many albums are represented in each Cluster?") +
  scale_y_continuous(breaks = seq(0, 10, 1)) +
  theme(axis.title.x = element_blank(), axis.text.x = element_blank())

# examine unique album count relative to cluster size
n_albums_prop_scaled <- df %>%
  group_by(cluster) %>%
  summarise(n_albums_prop = n_distinct(album_name) / n()) %>%
  ggplot(aes(factor(cluster), n_albums_prop)) +
  geom_col() +
  labs(x = "Cluster",
       y = "Proportion") +
  scale_x_discrete(labels = seq(1, 8, 1)) 

# display above plots with shared x-axis
grid::grid.newpage()
grid::grid.draw(
  rbind(ggplotGrob(albums_in_clusters_plot), ggplotGrob(n_albums_prop_scaled),
  size = "last"))
```

* No cluster contained tracks from all ``r n_distinct(df$album_name)`` albums
  within the dataset
* The smallest cluster, sized at just 3 tracks, contains tracks from 3 different
  albums. This is especially interesting, as it suggests that those three tracks
  are distinct enough from everything else to be their own cluster, yet similar
  enough to each other to be placed together, and all originated from different
  points in Mac's career
* Outside of the two smallest clusters, most other clusters contained a unique
  album proportion of ~0.25-0.30. Breaking this down further to investigate
  the balance of tracks per album included in each cluster could yield some 
  valuable insights

Let's take a look at how we can further assess each albums representation in
the clustering assignments.

```{r}
# generate vector of album names in order of release date
album_names <- df %>%
  select(album_name, album_release_date) %>%
  arrange(album_release_date) %>%
  unique() %>% .$album_name
```


```{r album-balance-within-clusters}
# create plot showing # of clusters reached by each album
df %>% 
  group_by(album_name, album_release_date) %>%
  summarise(count = n_distinct(cluster), .groups = "keep") %>%
  ggplot(aes(factor(album_release_date), count)) +
  geom_col(aes(fill = album_name), show.legend = FALSE) +
  scale_x_discrete(labels = stringr::str_wrap(album_names, 29)) +
  scale_fill_manual(values = album_palette) +
  labs(x = "",
       y = "# of Clusters Reached",
       title = "How many Clusters can each Album be found in?") +
  coord_flip()
```

This plot tells us that most of our clusters are pretty well-dispersed amongst
the clusters. There are a couple albums on the lower side, such as *Circles* and
*Best Day Ever*, which suggests the possibility that these albums contain tracks
that are more alike to each other when compared to other albums, 
like *Macadelic*. Additionally, it is worth noting that no album has a track in
every cluster, as the maximum measure shown above is 7 (remember, we have 8 
clusters in total). 

Below, we'll utilize the [plotly](https://plotly.com/) library to create an
interactive plot that allows for visualization of the album-to-cluster 
distributions. We'll be creating a alluvial plot to show the patterns.

```{r plotly-album-clust-plot, message = F}
library(plotly)
```

```{r prep-sankey-data}
# wrangle data to sankey (wide) format
sankey_data <- df %>%
  group_by(album_name, cluster) %>%
  summarise(count = n(), .groups = "keep")

# attack album color to sankey df for plotting
sankey_data$color = scales::alpha(
  unname(album_palette[sankey_data$album_name]), alpha = 0.5)
```

```{r album-cluster-sankey}
# create plotly sankey diagram
fig <- plot_ly(
  
  type = "sankey",
  orientation = "h",
  
  node = list(
    label = c(sort(album_names), 
              "cluster 1", "cluster 2", "cluster 3", "cluster 4",
              "cluster 5", "cluster 6", "cluster 7", "cluster 8"),
    color = c(unname(album_palette[order(factor(names(album_palette)))]), 
              rep("grey", 8)),
    pad = 15, 
    thickness = 20,
    line = list(
      color = "black",
      width = 0.5
    )
  ),
  
  link = list(
    source = as.numeric(factor(sankey_data$album_name)) - 1,
    target = as.numeric(factor(sankey_data$cluster)) + 10,
    value = sankey_data$count,
    color = sankey_data$color
  ),
  
  arrangement = "snap"
)

fig <- fig %>% layout(
  title = "Album-Cluster Relations",
  font = list(
    size = 12
  )
)

fig
```

This diagram reveals that the albums are pretty evenly distributed. There isn't
any cluster that is dominated by a single album, which is pretty surprising but
encouraging news! While not comprised mostly of a single album, its interesting
to note that more than 50% of cluster 6's tracks come from 
*The Divine Feminine*, *Swimming*, and *Circles (Deluxe)*, Mac's last three
released albums. This suggests that there exists a particular style that he 
developed on these albums that is present in each, and is alike enough to be
grouped by our clustering algorithm.

# Attribute Analysis

```{r create-boxplot-data}
boxplot_data <- df %>%
  mutate(cluster = as.factor(cluster)) %>%
  select(cluster, colnames(inputs))# %>%
  # pivot_longer(cols = colnames(inputs))
```

```{r attribute-boxplots}
fig <- boxplot_data %>% plot_ly(
  y = ~acousticness,
  x = ~cluster,
  type = "box"
  # transforms = list(
  #   list(
  #     type = "filter",
  #     target = ~name,
  #     operation = "=",
  #     value = colnames(inputs)[1]
  #   )
  # )
) %>% layout(
  title = "Attribute Boxplot by Cluster",
  yaxis = list(title = "Value"),
  updatemenus = list(
    list(
      y = 1,
      buttons = list(
        list(method = "restyle",
             args = list("y", list(boxplot_data$acousticness)),
                         label = "Acousticness"),
        list(method = "restyle",
             args = list("y", list(boxplot_data$danceability)),
                         label = "Danceability"),
        list(method = "restyle",
             args = list("y", list(boxplot_data$energy)),
                         label = "Energy"),
        list(method = "restyle",
             args = list("y", list(boxplot_data$instrumentalness)),
                         label = "Instrumentalness"),
        list(method = "restyle",
             args = list("y", list(boxplot_data$liveness)),
                         label = "Liveness"),
        list(method = "restyle",
             args = list("y", list(boxplot_data$loudness)),
                         label = "Loudness"),
        list(method = "restyle",
             args = list("y", list(boxplot_data$speechiness)),
                         label = "Speechiness"),
        list(method = "restyle",
             args = list("y", list(boxplot_data$valence)),
                         label = "Valence"),
        list(method = "restyle",
             args = list("y", list(boxplot_data$duration_ms)),
                         label = "Duration (ms)"),
        list(method = "restyle",
             args = list("y", list(boxplot_data$tempo)),
                         label = "Tempo")
        )
    )
  )
)
```

```{r}
fig
```

This plot lets us look at the statistical summary of each cluster for a desired
attribute. From clicking through the various attributes, we can see that
**danceability** seems to have the least variation across clusters. We can also
identify the main contributing attribute for the tracks in cluster 3, which is
likely **instrumentalness*. The value for cluster 3 is vastly different from all
other clusters.

# Conclusions

With all of the data clustered and sorted, we can now move on to the conclusion
section, allowing us to procure a summary of the procedure, identify some key
takeaways, and reflect on some limitations or areas to improve on moving
forward.

## Summary

* The goal of this analysis was to explore how Mac Miller's sound shifted over
  the course of his discography, and to attempt to identify any patterns or
  trends that may exist in his career, or in a specific album.
* We used data visualizations, statistical summary measurements, and K-means
  clustering to work through the data and search for interesting findings.
* This information could be used to help categorize previously unrelated songs
  into clusters of sonically similar tracks, which could be useful for dynamic
  playlist generation or track suggestions.
  
## Key Takeaways

* The ideal number of clusters was less than the number of albums included in
  the data, suggesting that on average, Mac's tracks aren't drastically
  different from each other across albums, which would've produced more clusters
  than albums to account for varying styles.
* Mac's most recent albums tended to have more attributes in common with each
  other than his earlier albums, suggesting that as his career went on, his
  style of sound became more defined and consistent.
  
## Reflections, Limitations, and Expansions

* Choosing to analyze a single artists' discography made it difficult to produce
  drastically different clusters. The mean values for each attribute in a
  cluster tended to not vary greatly, suggesting that the observations
  themselves, in this case the tracks, were usually more similar than they were
  different. To remedy this, it would be better to include a larger sample size
  that contains music from different genres and artists.
* The number of clusters was chosen based on the elbow method due to its
  popularity and ease of use. However, clustering attempts based on values
  chosen using the Gap and Silhouette methods would allow for a more thorough
  analysis.
* If the idea were to create playlists based on the clusters, it would be wise
  to set a floor number of observations per cluster, as a playlist of 3 tracks
  is not a very useful output for such a purpose. If needed, the number of
  clusters could be reduced in order to attain this property.
* A dashboard-like interface where a user could import a playlist or group of
  songs, see the audio features visualized, select their desired number of 
  clusters, and then explore the features and contents of each cluster could
  make for a very interesting and fun project. That seems like a natural
  progression for this sort of analysis, and would allow end users to interact
  with the clustering process while exploring music of their choice.
  
  That's all for now, thanks for reading!